# Stochastic-Gradient-Boosting-and-Adaboost

This repository is for CSCI 4961/6961 Project Details, Fall 2020. This code is an extension of the research provided in the paper Stochastic Gradient Boosting by Jerome H. Friedman (https://statweb.stanford.edu/~jhf/ftp/stobst.pdf) 

In this repository we evaluate the performance of Stochastic Boosting and Traditional Boosting methods in two ways. The first is through evaluating the amount of data needed for each method to effectively generalize the classification problem. The second is effect of increasing the complexity of Weak Learner. How does a Weak Learner perform as it becomes for complex? Is it still able to generalize the classification problem in the same number of epochs?

## Files

  **** - Test code for the homework assignment

  **Stochastic Gradient Boosting.pdf** - Slide Show
  
  **Randomized_Algorithms_for_Machine_Learning_and_Optimization_Project_HW.pdf** - Homework PDF
  
  **Experiment_Data.pdf** - Experiment Results and Graphs (Link to Google Sheets Above)
  
  **accent_data.csv** - Data set for Speaker Accent Recognition
  
  **deepfakes_data.csv** - Data set for Medical Image Tamper Detection
  
  **experiment.ipynb** - Code for running analysis on experiment
  
  **functions.py** - Contains Boost and GradientDescent classes

## Directories

  **data** - Directory containing data files
