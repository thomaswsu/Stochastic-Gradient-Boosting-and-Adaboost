# Stochastic-Gradient-Boosting-and-Adaboost

This repository is for CSCI 4961/6961 Project Details, Fall 2020. This code is an extension of the research provided in the paper Stochastic Gradient Boosting by Jerome H. Friedman (https://statweb.stanford.edu/~jhf/ftp/stobst.pdf) 

In this repository we evaluate the performance of Stochastic Boosting and Traditional Boosting methods in two ways. The first is through evaluating the amount of data needed for each method to effectively generalize the classification problem. The second is effect of increasing the complexity of Weak Learner. How does a Weak Learner perform as it becomes for complex? Is it still able to generalize the classification problem in the same number of epochs?

## data/Homework

This directory contains the datasets used in the homework

### Directories

  **Accent_Recognition** - Contains data for Accent_Recognition dataset
  
  **Medical_Deepfakes** - Contains data for Medical_Deepfakes dataset
