# Stochastic-Gradient-Boosting-and-Adaboost

This repository is for CSCI 4961/6961 Project Details, Fall 2020. This code is an extension of the research provided in the paper Stochastic Gradient Boosting by Jerome H. Friedman (https://statweb.stanford.edu/~jhf/ftp/stobst.pdf) 

In this repository we evaluate the performance of Stochastic Boosting, Traditional Boosting, Gradient Descent, and Stochastic Gradient Descent methods in two ways. The first is through evaluating the amount of data needed for each method to effectively generalize the classification problem. The second is effect of increasing the complexity of Weak Learner. How does a Weak Learner perform as it becomes for complex? Is it still able to generalize the classification problem in the same number of epochs?

## data

This directory holds the data files necessary to run experiments and test homework solutions

### Directories

  **Chinese_MNIST** - Directory containing data for Chinese_MNIST
  
  **Homework** - Directory containing data for Homework datasets
  
  **Sign_Language_MNIST** - Directory containing data for Sign_Language_MNIST
